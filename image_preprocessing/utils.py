import cv2
import numpy as np
from enum import Enum
import os
try:
    import pytesseract
except ImportError:
    pytesseract = None


class ImageType(Enum):
    """Typ vstupnÃ­ho obrÃ¡zku"""
    SCREENSHOT = "screenshot"
    PHOTO = "photo"
    SCAN = "scan"


def detect_image_type(image: np.ndarray, image_path: str = None) -> ImageType:
    """
    Detekuje typ obrÃ¡zku (screenshot, photo, scan).

    Heuristiky:
    - Screenshot: obvykle mÃ¡ ostrÃ½ text, vysokÃ½ kontrast, Å¾Ã¡dnÃ© perspektivnÃ­ zkreslenÃ­
    - Photo: mÅ¯Å¾e bÃ½t rozmazanÃ½, mÃ¡ perspektivnÃ­ zkreslenÃ­, rÅ¯znÃ© osvÄ›tlenÃ­
    - Scan: vysoce kvalitnÃ­, rovnÃ½, Äasto mÃ¡ ÄernÃ½ okraj

    Args:
        image: VstupnÃ­ obrÃ¡zek
        image_path: Cesta k obrÃ¡zku (pouÅ¾ije se pro detekci podle nÃ¡zvu sloÅ¾ky)

    Returns:
        Typ obrÃ¡zku
    """
    # Pokud je cesta k obrÃ¡zku, zkusÃ­me detekci podle sloÅ¾ky
    if image_path:
        path_lower = image_path.lower()
        if 'screenshot' in path_lower:
            return ImageType.SCREENSHOT
        elif 'photo' in path_lower:
            return ImageType.PHOTO
        elif 'scan' in path_lower:
            return ImageType.SCAN

    # HeuristickÃ¡ detekce podle vlastnostÃ­ obrÃ¡zku
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

    # VÃ½poÄet Laplacian variance (mÃ­ra ostrosti)
    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()

    # Screenshots majÃ­ obvykle velmi ostrÃ½ text
    if laplacian_var > 1000:
        return ImageType.SCREENSHOT

    # Detekce okrajÅ¯ pro identifikaci papÃ­ru (photos)
    edges = cv2.Canny(gray, 50, 150)
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Pokud najdeme velkÃ½ ÄtyÅ™ÃºhelnÃ­k, pravdÄ›podobnÄ› jde o fotku papÃ­ru
    if len(contours) > 0:
        largest_contour = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(largest_contour)
        image_area = image.shape[0] * image.shape[1]

        # Pokud nejvÄ›tÅ¡Ã­ kontura zabÃ­rÃ¡ 30-95% obrÃ¡zku, mÅ¯Å¾e jÃ­t o papÃ­r na fotce
        if 0.3 < area / image_area < 0.95:
            peri = cv2.arcLength(largest_contour, True)
            approx = cv2.approxPolyDP(largest_contour, 0.02 * peri, True)
            if len(approx) == 4:
                return ImageType.PHOTO

    # VÃ½chozÃ­: SCAN
    return ImageType.SCAN


def order_points(pts: np.ndarray) -> np.ndarray:
    """
    SeÅ™adÃ­ 4 body ÄtyÅ™ÃºhelnÃ­ku v poÅ™adÃ­: top-left, top-right, bottom-right, bottom-left.

    Args:
        pts: Array 4 bodÅ¯ tvaru (4, 2)

    Returns:
        SeÅ™azenÃ© body
    """
    rect = np.zeros((4, 2), dtype="float32")

    # Top-left mÃ¡ nejmenÅ¡Ã­ souÄet, bottom-right nejvÄ›tÅ¡Ã­
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]

    # Top-right mÃ¡ nejmenÅ¡Ã­ rozdÃ­l, bottom-left nejvÄ›tÅ¡Ã­
    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]

    return rect


def four_point_transform(image: np.ndarray, pts: np.ndarray) -> np.ndarray:
    """
    Provede perspektivnÃ­ transformaci na zÃ¡kladÄ› 4 bodÅ¯.

    Args:
        image: VstupnÃ­ obrÃ¡zek
        pts: 4 body definujÃ­cÃ­ ÄtyÅ™ÃºhelnÃ­k

    Returns:
        TransformovanÃ½ obrÃ¡zek (pohled shora)
    """
    rect = order_points(pts)
    (tl, tr, br, bl) = rect

    # VÃ½poÄet Å¡Ã­Å™ky vÃ½stupnÃ­ho obrÃ¡zku
    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
    maxWidth = max(int(widthA), int(widthB))

    # VÃ½poÄet vÃ½Å¡ky vÃ½stupnÃ­ho obrÃ¡zku
    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
    maxHeight = max(int(heightA), int(heightB))

    # CÃ­lovÃ© body pro transformaci
    dst = np.array([
        [0, 0],
        [maxWidth - 1, 0],
        [maxWidth - 1, maxHeight - 1],
        [0, maxHeight - 1]
    ], dtype="float32")

    # VÃ½poÄet perspektivnÃ­ transformace
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))

    return warped


def rotate_image(image: np.ndarray, angle: float) -> np.ndarray:
    """
    OtoÄÃ­ obrÃ¡zek o danÃ½ Ãºhel.

    Args:
        image: VstupnÃ­ obrÃ¡zek
        angle: Ãšhel v stupnÃ­ch (kladnÃ½ = proti smÄ›ru hodinovÃ½ch ruÄiÄek)

    Returns:
        OtoÄenÃ½ obrÃ¡zek
    """
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)

    # VytvoÅ™enÃ­ rotaÄnÃ­ matice
    M = cv2.getRotationMatrix2D(center, angle, 1.0)

    # VÃ½poÄet novÃ½ch rozmÄ›rÅ¯
    cos = np.abs(M[0, 0])
    sin = np.abs(M[0, 1])
    new_w = int((h * sin) + (w * cos))
    new_h = int((h * cos) + (w * sin))

    # Ãšprava translace
    M[0, 2] += (new_w / 2) - center[0]
    M[1, 2] += (new_h / 2) - center[1]

    # Rotace
    rotated = cv2.warpAffine(image, M, (new_w, new_h),
                             flags=cv2.INTER_CUBIC,
                             borderMode=cv2.BORDER_CONSTANT,
                             borderValue=(255, 255, 255))

    return rotated


def detect_rotation_angle(image: np.ndarray, debug: bool = False) -> float:
    """
    Detekuje Ãºhel rotace textu v obrÃ¡zku pomocÃ­ analÃ½zy textovÃ½ch komponent.

    KlÃ­ÄovÃ¡ zmÄ›na: PoÄÃ­tÃ¡ Ãºhel z TEXTU (malÃ½ch komponent), ne z celÃ©ho obrazu/pozadÃ­.
    TÃ­mto se vyhne detekci hran stolu, kachliÄek, prken apod.

    Args:
        image: VstupnÃ­ obrÃ¡zek (grayscale)
        debug: Pokud True, vypÃ­Å¡e debug informace

    Returns:
        Ãšhel rotace ve stupnÃ­ch
    """
    # Pokud je obrÃ¡zek barevnÃ½, pÅ™evedeme na grayscale
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image

    h, w = gray.shape[:2]

    # PouÅ¾ij celÃ½ obrÃ¡zek (YOLO uÅ¾ udÄ›lal crop)
    center_roi = gray

    # KROK 2: VytvoÅ™ masku textu pomocÃ­ adaptive threshold
    # Invertuj, aby text byl bÃ­lÃ½ (255), pozadÃ­ ÄernÃ© (0)
    binary = cv2.adaptiveThreshold(
        center_roi, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV, 11, 2
    )

    # KROK 3: Najdi textovÃ© komponenty (connected components)
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary, connectivity=8)

    if num_labels <= 1:  # Pouze pozadÃ­
        return 0.0

    # KROK 4: Filtruj komponenty podle plochy
    # ZahoÄ obrovskÃ© bloby (pozadÃ­, dlouhÃ© ÄÃ¡ry) a malÃ© Å¡umy
    # NechÃ¡me jen stÅ™ednÃ­ komponenty (pravdÄ›podobnÄ› pÃ­smena)
    roi_area = center_roi.shape[0] * center_roi.shape[1]
    min_area = roi_area * 0.0001  # 0.01% plochy ROI
    max_area = roi_area * 0.05    # 5% plochy ROI (velkÃ© ÄÃ¡ry/pozadÃ­)

    text_pixels = []
    for i in range(1, num_labels):  # Skip label 0 (background)
        area = stats[i, cv2.CC_STAT_AREA]
        if min_area < area < max_area:
            # PÅ™idej vÅ¡echny pixely tÃ©to komponenty
            mask = (labels == i).astype(np.uint8) * 255
            coords = cv2.findNonZero(mask)
            if coords is not None:
                text_pixels.append(coords.reshape(-1, 2))

    if len(text_pixels) == 0:
        if debug:
            print(f"  âš ï¸  No text components found for rotation detection")
        return 0.0

    # SpojÃ­me vÅ¡echny textovÃ© pixely dohromady
    all_text_pixels = np.vstack(text_pixels)

    # KROK 5: PouÅ¾ij PCA (Principal Component Analysis) pro detekci hlavnÃ­ho smÄ›ru textu
    # PCA najde hlavnÃ­ smÄ›r rozloÅ¾enÃ­ textovÃ½ch pixelÅ¯
    try:
        # VlastnÃ­ implementace PCA pomocÃ­ numpy
        # Centrum dat
        mean = np.mean(all_text_pixels, axis=0)
        centered = all_text_pixels - mean

        # KovarianÄnÃ­ matice
        cov_matrix = np.cov(centered.T)

        # VlastnÃ­ ÄÃ­sla a vlastnÃ­ vektory
        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

        # SeÅ™aÄ podle velikosti vlastnÃ­ch ÄÃ­sel (sestupnÄ›)
        idx = eigenvalues.argsort()[::-1]
        eigenvectors = eigenvectors[:, idx]

        # PrvnÃ­ vlastnÃ­ vektor (hlavnÃ­ komponenta)
        eigen_vec = eigenvectors[:, 0]

        # VÃ½poÄet Ãºhlu z eigenvektoru
        angle = np.degrees(np.arctan2(eigen_vec[1], eigen_vec[0]))

        # Normalizuj na rozsah -45 aÅ¾ +45
        if angle < -45:
            angle = angle + 90
        elif angle > 45:
            angle = angle - 90

        if debug:
            print(f"  ğŸ“ Detected rotation angle from text: {angle:.2f}Â° (PCA)")

        return angle

    except Exception as e:
        if debug:
            print(f"  âš ï¸  PCA failed: {e}, using fallback")

    # Fallback na minAreaRect (pokud PCA selÅ¾e)
    rect = cv2.minAreaRect(all_text_pixels)
    ((cx, cy), (width, height), angle) = rect

    if width < height:
        angle = angle - 90

    if angle < -45:
        angle = angle + 90
    elif angle > 45:
        angle = angle - 90

    if debug:
        print(f"  ğŸ“ Detected rotation angle from text: {angle:.2f}Â° (minAreaRect fallback)")

    return angle


def _check_180_rotation(gray: np.ndarray, debug: bool = False) -> bool:
    """
    Zkontroluje, zda je text otoÄenÃ½ o 180Â° pomocÃ­ OCR testu.

    Args:
        gray: Grayscale obrÃ¡zek
        debug: Pokud True, vypÃ­Å¡e debug informace

    Returns:
        True pokud je text otoÄenÃ½ o 180Â°, False jinak
    """
    if pytesseract is None:
        return False

    try:
        # Crop do stÅ™edu obrÃ¡zku (pro rychlejÅ¡Ã­ OCR)
        h, w = gray.shape[:2]
        crop_h = min(h // 2, 400)
        crop_w = min(w // 2, 400)
        y_start = (h - crop_h) // 2
        x_start = (w - crop_w) // 2
        cropped = gray[y_start:y_start+crop_h, x_start:x_start+crop_w]

        # OCR na aktuÃ¡lnÃ­ orientaci
        data_0 = pytesseract.image_to_data(cropped, output_type=pytesseract.Output.DICT, lang='ces')
        conf_0 = [float(c) for c in data_0['conf'] if c != '-1']
        avg_conf_0 = sum(conf_0) / len(conf_0) if conf_0 else 0

        # OCR na rotovanÃ© o 180Â°
        rotated = cv2.rotate(cropped, cv2.ROTATE_180)
        data_180 = pytesseract.image_to_data(rotated, output_type=pytesseract.Output.DICT, lang='ces')
        conf_180 = [float(c) for c in data_180['conf'] if c != '-1']
        avg_conf_180 = sum(conf_180) / len(conf_180) if conf_180 else 0

        if debug:
            print(f"  ğŸ”„ 180Â° check: current={avg_conf_0:.1f}, rotated={avg_conf_180:.1f}")

        # Pokud je rotovanÃ© vÃ½raznÄ› lepÅ¡Ã­ (20% rozdÃ­l), text je otoÄenÃ½ o 180Â°
        return avg_conf_180 > avg_conf_0 * 1.2

    except Exception as e:
        if debug:
            print(f"  âš ï¸  180Â° check failed: {e}")
        return False


def detect_text_orientation(image: np.ndarray, debug: bool = False) -> int:
    """
    Detekuje orientaci textu v obrÃ¡zku pomocÃ­ Tesseract OSD.

    Args:
        image: VstupnÃ­ obrÃ¡zek
        debug: Pokud True, vypÃ­Å¡e debug informace

    Returns:
        Ãšhel rotace potÅ™ebnÃ½ k narovnÃ¡nÃ­ textu (0, 90, 180, 270)
    """
    # PÅ™evod na grayscale pokud je potÅ™eba
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image

    h, w = gray.shape[:2]

    if debug:
        print(f"  ğŸ” Image dimensions: {w}x{h} (aspect ratio: {w/h:.2f})")

    if pytesseract is None:
        # Fallback: heuristika zaloÅ¾enÃ¡ na aspect ratio
        if h > w * 1.3:  # Pokud je vÃ½raznÄ› vyÅ¡Å¡Ã­ neÅ¾ Å¡irÅ¡Ã­
            if debug:
                print(f"  âš ï¸  No pytesseract, using heuristic: rotate 270Â° (or -90Â°)")
            return 270  # OtoÄit o 270Â° = -90Â° = doprava po smÄ›ru hodinovÃ½ch ruÄiÄek
        return 0

    try:
        # Tesseract OSD (Orientation and Script Detection)
        # --psm 0 = pouze OSD, Å¾Ã¡dnÃ© OCR
        use_fallback = False

        try:
            osd = pytesseract.image_to_osd(gray)

            if debug:
                print(f"  ğŸ“„ Tesseract OSD output:")
                for line in osd.split('\n'):
                    if line.strip():
                        print(f"     {line}")

            # Parse vÃ½stupu
            rotation_angle = 0
            orientation_conf = 0
            for line in osd.split('\n'):
                if 'Rotate:' in line:
                    rotation_angle = int(line.split(':')[1].strip())
                if 'Orientation confidence:' in line:
                    orientation_conf = float(line.split(':')[1].strip())

            if debug:
                print(f"  ğŸ”„ Detected rotation: {rotation_angle}Â° (confidence: {orientation_conf:.1f})")

            # Pouze pokud je confidence dostateÄnÄ› vysokÃ¡ (min 1.5), pouÅ¾ijeme Tesseract vÃ½sledek
            if orientation_conf >= 1.5:
                # SpeciÃ¡lnÃ­ pÅ™Ã­pad: Pokud Tesseract Å™ekl 0Â°, zkontroluj 180Â°
                # (protoÅ¾e Tesseract nÄ›kdy nedokÃ¡Å¾e rozliÅ¡it 0Â° od 180Â°)
                if rotation_angle == 0:
                    if _check_180_rotation(gray, debug):
                        if debug:
                            print(f"  ğŸ”„ OCR confidence better at 180Â°, overriding Tesseract")
                        return 180
                return rotation_angle
            else:
                if debug:
                    print(f"  âš ï¸  Confidence too low ({orientation_conf:.1f} < 1.5), using Hough fallback")
                use_fallback = True

        except Exception as e:
            if debug:
                print(f"  âš ï¸  Tesseract OSD failed: {e}")
            use_fallback = True

        # Fallback: PouÅ¾ijeme Hough detekci liniÃ­ k urÄenÃ­ orientace
        # (provede se kdyÅ¾ Tesseract selÅ¾e NEBO mÃ¡ nÃ­zkou confidence)
        if use_fallback:
            edges = cv2.Canny(gray, 50, 150, apertureSize=3)
            lines = cv2.HoughLines(edges, 1, np.pi / 180, 100)

            if lines is not None and len(lines) > 5:
                # SpoÄÃ­tÃ¡me horizontÃ¡lnÃ­ vs vertikÃ¡lnÃ­ linie
                horizontal_count = 0
                vertical_count = 0

                for rho, theta in lines[:, 0]:
                    angle_deg = (theta * 180 / np.pi)

                    # HorizontÃ¡lnÃ­ linie: kolem 0Â° nebo 180Â°
                    if (angle_deg < 20 or angle_deg > 160):
                        horizontal_count += 1
                    # VertikÃ¡lnÃ­ linie: kolem 90Â°
                    elif (70 < angle_deg < 110):
                        vertical_count += 1

                if debug:
                    print(f"  ğŸ“ Line detection: {horizontal_count} horizontal, {vertical_count} vertical")

                # Pokud je vÃ­c vertikÃ¡lnÃ­ch neÅ¾ horizontÃ¡lnÃ­ch, text je otoÄenÃ½ o 90Â°
                # PouÅ¾ijeme niÅ¾Å¡Ã­ threshold (1.2x mÃ­sto 1.5x) pro lepÅ¡Ã­ detekci
                if vertical_count > horizontal_count * 1.2:
                    if debug:
                        print(f"  ğŸ“ Detected vertical text, rotating 270Â° (or -90Â°)")
                    return 270

            # PoslednÃ­ fallback: aspect ratio
            if h > w * 1.3:
                if debug:
                    print(f"  ğŸ“ Using aspect ratio heuristic (h>w*1.3): rotate 270Â° (or -90Â°)")
                return 270

            # FinÃ¡lnÃ­ fallback: 180Â° OCR test
            if _check_180_rotation(gray, debug):
                if debug:
                    print(f"  ğŸ”„ OCR confidence better at 180Â°, rotating")
                return 180

            if debug:
                print(f"  ğŸ“ No clear orientation detected, keeping 0Â°")
            return 0

    except Exception as e:
        if debug:
            print(f"  âŒ Error: {e}")
        # Fallback: Å¾Ã¡dnÃ¡ rotace
        return 0
